{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analyzing Song Lyrics using scikit-learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load song lyrics dataset.\n",
    "import json\n",
    "song_lyrics_2 = json.load(open('data\\song_lyrics_2.json','rt'), encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_lyrics ={tuple(tup[0]):tup[1] for tup in song_lyrics_2[2000:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the titles and lyrics into two lists.\n",
    "titles, lyrics = zip(*song_lyrics.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the CountVectorizer.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Create a count matrix from the list of documents.\n",
    "lyrics_matrix = cv.fit_transform(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of a TfidfTransformer.\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Created a weighted matrix.\n",
    "weighted_matrix = tfidf.fit_transform(lyrics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute distance matrix.\n",
    "distance_matrix = pairwise_distances(lyrics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest_pairs(dist_mat, names, num=10, ignore_zeros=False):\n",
    "    \"\"\"Finds pairs with closest non-zero distances.\"\"\"\n",
    "    dist_mat = np.copy(dist_mat)\n",
    "    # Set zero values to infinity so they are ignore in the partitioning.\n",
    "    dist_mat[np.triu_indices_from(dist_mat)] = np.inf\n",
    "    if ignore_zeros:\n",
    "        dist_mat[dist_mat == 0] = np.inf\n",
    "    # Unravel the matrix indices and partition num lowest numbers.\n",
    "    unr_index = np.unravel_index(dist_mat.argpartition(num, axis=None), dist_mat.shape)\n",
    "    # Get document names for pairs.\n",
    "    dist_pairs = [(names[unr_index[0][i]],        # First doc\n",
    "                   names[unr_index[1][i]],        # Second doc\n",
    "                   dist_mat[unr_index[0][i], unr_index[1][i]])   # Distance between docs\n",
    "                   for i in xrange(num)]                # First num indices\n",
    "    return sorted(dist_pairs, key=lambda tup: tup[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the 25 closests \n",
    "closest = get_closest_pairs(distance_matrix, titles, num=25)\n",
    "for tup in closest:\n",
    "    print(str(tup[0][2]) + ', ' + str(tup[1][2]) + '  :  ' + str(tup[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(closest)):\n",
    "    title1, title2 = closest[i][0], closest[i][1]\n",
    "    two_cols = izip_longest(lyrics[titles.index(title1)].encode('utf8').split('\\n'), \n",
    "                            lyrics[titles.index(title2)].encode('utf8').split('\\n'), fillvalue='')\n",
    "    print('____{0:46} | ____{1}'.format(title1[2], title2[2]))\n",
    "    for tup in two_cols:\n",
    "        #print('{0:50} | {1}'.format(*map(tup))\n",
    "        print('{0:50} | {1}'.format(*tup))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>These are all instrumentals without lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " a= [k for k,v in song_lyrics.items() if 'Instrumental' in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out songs that do not have lyrics in the dataset.\n",
    "song_lyrics_filtered = {k:v for k,v in song_lyrics.items() if 'Instrumental' in v[:20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the titles and lyrics into two lists.\n",
    "titles, lyrics = zip(*song_lyrics_filtered.items())\n",
    "\n",
    "# Create an instance of the CountVectorizer.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Create a count matrix from the list of documents.\n",
    "lyrics_matrix = cv.fit_transform(lyrics)\n",
    "\n",
    "# Create an instance of a TfidfTransformer.\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Created a weighted matrix.\n",
    "weighted_matrix = tfidf.fit_transform(lyrics_matrix)\n",
    "\n",
    "# Compute distance matrix.\n",
    "distance_matrix = pairwise_distances(lyrics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the 25 closests \n",
    "closest = get_closest_pairs(distance_matrix, titles, num=25)\n",
    "for tup in closest:\n",
    "    print(str(tup[0][2]) + ', ' + str(tup[1][2]) + '  :  ' + str(tup[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(closest)):\n",
    "    title1, title2 = closest[i][0], closest[i][1]\n",
    "    two_cols = izip_longest(lyrics[titles.index(title1)].encode('utf8').split('\\n'), \n",
    "                            lyrics[titles.index(title2)].encode('utf8').split('\\n'), fillvalue='')\n",
    "    print('____{0:46} | ____{1}'.format(title1[2], title2[2]))\n",
    "    for tup in two_cols:\n",
    "        #print('{0:50} | {1}'.format(*map(tup))\n",
    "        print('{0:50} | {1}'.format(*tup))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** This may not be true anymore***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a number of songs appear in the dataset more than once, sometimes with different names. In fact these songs appear in the top 100 songs from multiple years, which is why they show up more than one. Because they titles are slightly different, the dict created new entries instead of over-riding the values of an existing key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's ignore all pairs with a distance of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the 10 closests \n",
    "closest = get_closest_pairs(distance_matrix, titles, num=10, ignore_zeros=True)\n",
    "for tup in closest:\n",
    "    print(str(tup[0][2]) + ', ' + str(tup[1][2]) + '  :  ' + str(tup[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these are our closest matches. Let's see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out closest matching pairs and their lyrics\n",
    "for i in xrange(len(closest)):\n",
    "    title1, title2 = closest[i][0], closest[i][1]\n",
    "    print(str(title1) + ' : ' + lyrics[titles.index(title1)])\n",
    "    print(str(title2) + ' : ' + lyrics[titles.index(title2)])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another instance where TFIDF has found something we didn't expect. Let's remove these songs from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_remove = [x for x in lyrics if x.startswith('We do not have') \n",
    "                                or x.startswith('[Instrumental]')\n",
    "                                or x.startswith('Instrumental')\n",
    "                                or x.startswith('Sorry, we have no')]\n",
    "to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out songs that do not have lyrics in the dataset.\n",
    "song_lyrics_filtered = {k:v for k,v in song_lyrics.items() if not (v.startswith('We do not have')\n",
    "                                                                   or v.startswith('Sorry, we have no')\n",
    "                                                                   or 'Instrumental' in v[:20])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An interesting example: Attempt #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the titles and lyrics into two lists.\n",
    "titles, lyrics = zip(*song_lyrics_filtered.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the CountVectorizer.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Create a count matrix from the list of documents.\n",
    "lyrics_matrix = cv.fit_transform(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of a TfidfTransformer.\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Created a weighted matrix.\n",
    "weighted_matrix = tfidf.fit_transform(lyrics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the matrix memory footprint smaller by changing the dtype.\n",
    "weighted_matrix = weighted_matrix.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute distance matrix.\n",
    "distance_matrix = pairwise_distances(lyrics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the 10 closests \n",
    "closest = get_closest_pairs(distance_matrix, titles, num=20, ignore_zeros=True)\n",
    "for tup in closest:\n",
    "    print(str(tup[0][2]) + ', ' + str(tup[1][2]) + '  :  ' + str(tup[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(closest)):\n",
    "    title1, title2 = closest[i][0], closest[i][1]\n",
    "    two_cols = izip_longest(lyrics[titles.index(title1)].encode('utf8').split('\\n'), \n",
    "                            lyrics[titles.index(title2)].encode('utf8').split('\\n'), fillvalue='')\n",
    "    print('____{0:46} | ____{1}'.format(title1[2], title2[2]))\n",
    "    for tup in two_cols:\n",
    "        #print('{0:50} | {1}'.format(*map(tup))\n",
    "        print('{0:50} | {1}'.format(*tup))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find songs that appear most often in closest pairs.\n",
    "closest_50 = get_closest_pairs(distance_matrix, titles, num=50, ignore_zeros=True)\n",
    "\n",
    "count = Counter([t[2] for tup in closest_50 for t in tup[:2]])\n",
    "\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count2 = Counter([t for tup in closest_50 for t in tup[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count2.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lyrics[titles.index(count2.most_common(1)[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You Must Love Me is getting repeated a lot. Let's take a look at the terms in the song and their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the index of a song in the titles list.\n",
    "#titles.index('You Must Love Me Lyrics  Madonna')\n",
    "titles.index('Piece Of My Heart Lyrics  Tara Kemp')\n",
    "# This index corresponds the the row for this song in the matrix.\n",
    "ind = np.nonzero(weighted_matrix[754])[1]\n",
    "# Create a term to index dict.\n",
    "t = zip(*cv.vocabulary_.items())\n",
    "term_ind = dict(zip(t[1], t[0]))\n",
    "sorted([(weighted_matrix[754, i], term_ind[i]) for i in ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term with the most weight is \"chorus.\" Sometimes the most weighted term is a strong indicator of related documents. In this case, however, all songs have a chorus, and some of the songs have labeled it in their lyrics. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(song_lyrics_filtered['Piece Of My Heart Lyrics  Tara Kemp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An interesting example: Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we deal with words like \"chorus\" that appear in most/all documents (or even a few documents), but do not help us distinguish between different kinds of documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the CountVectorizer documentation to get a little more detail on the stop_words parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the titles and lyrics into two lists.\n",
    "titles, lyrics = zip(*song_lyrics_filtered.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the CountVectorizer.\n",
    "#cv = CountVectorizer(stop_words='english')\n",
    "custom_stop_words = list(ENGLISH_STOP_WORDS) + ['chorus']\n",
    "cv = CountVectorizer(stop_words=custom_stop_words)\n",
    "\n",
    "# Create a count matrix from the list of documents.\n",
    "lyrics_matrix = cv.fit_transform(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of a TfidfTransformer.\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Created a weighted matrix.\n",
    "weighted_matrix = tfidf.fit_transform(lyrics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the matrix memory footprint smaller by changing the dtype.\n",
    "weighted_matrix = weighted_matrix.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute distance matrix.\n",
    "distance_matrix = pairwise_distances(lyrics_matrix).astype(np.float16)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the 10 closests \n",
    "closest = get_closest_pairs(distance_matrix, titles, num=20, ignore_zeros=True)\n",
    "for tup in closest:\n",
    "    print(tup[0] + ', ' + tup[1] + '  :  ' + str(tup[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still seeing a lot of Tara Kemp and Madonna. Let's look at the lyrics more closely."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def get_weighted_vocab(name, names, matrix, cv):\n",
    "    # Get the index of a song in the titles list.\n",
    "    name_index = names.index(name)\n",
    "    # This index corresponds the the row for this song in the matrix.\n",
    "    # Get indices for all non-zero elements in the document vector.\n",
    "    ind = np.nonzero(weighted_matrix[name_index])[1]\n",
    "    # Create a term to index dict.\n",
    "    t = zip(*cv.vocabulary_.items())\n",
    "    term_ind = dict(zip(t[1], t[0]))\n",
    "    return sorted([(weighted_matrix[name_index, i], term_ind[i]) for i in ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_vocab(name, names, matrix, cv):\n",
    "    # Get the index of a song in the titles list.\n",
    "    name_index = names.index(name)\n",
    "    # This index corresponds the the row for this song in the matrix.\n",
    "    # Get indices for all non-zero elements in the document vector.\n",
    "    ind = np.nonzero(matrix[name_index])[1]\n",
    "    # Create a term to index dict.\n",
    "    t = zip(*cv.vocabulary_.items())\n",
    "    term_ind = dict(zip(t[1], t[0]))\n",
    "    return sorted([(matrix[name_index, i], term_ind[i]) for i in ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tara = get_weighted_vocab('Piece Of My Heart Lyrics  Tara Kemp', titles, weighted_matrix, cv)\n",
    "madonna = get_weighted_vocab('You Must Love Me Lyrics  Madonna', titles, weighted_matrix, cv)\n",
    "{tup[1] for tup in tara}.intersection({tup[1] for tup in madonna})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tara, madonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(closest)):\n",
    "    title1, title2 = closest[i][0], closest[i][1]\n",
    "    two_cols = izip_longest(lyrics[titles.index(title1)].split('\\n'), lyrics[titles.index(title2)].split('\\n'), fillvalue='')\n",
    "    print('____{0:46} | ____{1}'.format(title1, title2))\n",
    "    for tup in two_cols:\n",
    "        print('{0:50} | {1}'.format(*tup))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min(tfidf.idf_), max(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip(cv.vocabulary_, tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([\"I'd like an apple\",\n",
    "                             \"An apple a day keeps the doctor away\",\n",
    "                             \"Never compare an apple to an orange\",\n",
    "                             \"I prefer scikit-learn to Orange\"])\n",
    "(tfidf * tfidf.T).A\n",
    "array([[ 1.        ,  0.25082859,  0.39482963,  0.        ],\n",
    "       [ 0.25082859,  1.        ,  0.22057609,  0.        ],\n",
    "       [ 0.39482963,  0.22057609,  1.        ,  0.26264139],\n",
    "       [ 0.        ,  0.        ,  0.26264139,  1.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([\"I'd like an apple\",\n",
    "                             \"An apple a day keeps the doctor away\",\n",
    "                             \"Never compare an apple to an orange\",\n",
    "                             \"I prefer scikit-learn to Orange\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairwise_distances(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dists = distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html\n",
    "TextBlob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
